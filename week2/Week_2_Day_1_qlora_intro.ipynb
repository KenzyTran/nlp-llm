{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "## Dự đoán Giá Sản Phẩm\n",
        "\n",
        "### Tuần 2 Ngày 1\n",
        "\n",
        "Giới thiệu về LoRA và QLoRA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Một lưu ý về 2 mẹo quan trọng khi sử dụng Colab:\n",
        "\n",
        "**Mẹo 1:**\n",
        "\n",
        "Phía trên cùng của mỗi colab đều có một số lệnh pip install. Bạn có thể gặp phải lỗi từ pip khi chạy các lệnh này, ví dụ như:\n",
        "\n",
        "> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
        "\n",
        "Các lỗi tương thích pip này có thể bỏ qua an toàn; và mặc dù bạn có thể muốn sửa chúng bằng cách thay đổi số phiên bản, nhưng điều đó thực sự sẽ gây ra những vấn đề nghiêm trọng!\n",
        "\n",
        "**Mẹo 2:**\n",
        "\n",
        "Trong quá trình chạy Colab, bạn có thể gặp lỗi như sau:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "Đây là một thông báo lỗi rất dễ gây hiểu nhầm! Đừng cố thay đổi phiên bản của các gói...\n",
        "\n",
        "Thực chất, lỗi này xảy ra vì Google đã thay đổi runtime của bạn, có thể do Google Colab quá tải. Cách giải quyết là:\n",
        "\n",
        "1. Vào menu Kernel >> Disconnect and delete runtime\n",
        "2. Tải lại colab từ đầu và vào menu Edit >> Clear All Outputs\n",
        "3. Kết nối lại với T4 mới bằng nút ở góc trên bên phải\n",
        "4. Chọn \"View resources\" từ menu trên cùng bên phải để xác nhận bạn đã có GPU\n",
        "5. Chạy lại tất cả các cell trong colab, bắt đầu từ lệnh pip install ở trên cùng\n",
        "\n",
        "Và mọi thứ sẽ hoạt động tốt – nếu không, hãy hỏi mình nhé!"
      ],
      "metadata": {
        "id": "d14UgrL83gcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q datasets requests torch peft bitsandbytes transformers trl accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "# Hyperparameters for QLoRA Fine-Tuning\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Đăng nhập vào HuggingFace\n",
        "\n",
        "Nếu bạn chưa có tài khoản HuggingFace, hãy truy cập https://huggingface.co để đăng ký và tạo một token.\n",
        "\n",
        "Sau đó chọn mục Secrets cho Notebook này bằng cách nhấn vào biểu tượng hình chìa khóa ở bên trái, và thêm một secret mới tên là `HF_TOKEN` với giá trị là token của bạn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Thử nghiệm các phương pháp Lượng tử hóa khác nhau\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Base Model without quantization\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "ElUPTnB28iNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2VcoS_DY9YSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "j6gJWw3r86KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "sVf8hf6S88C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Khởi động lại phiên làm việc của bạn!\n",
        "\n",
        "Để tải mô hình tiếp theo và xóa bộ nhớ đệm của mô hình trước đó, bạn cần vào Runtime >> Restart session và chạy lại các cell ban đầu (cài đặt, import và đăng nhập HuggingFace).\n",
        "\n",
        "Việc này giúp làm sạch GPU."
      ],
      "metadata": {
        "id": "Gv-00uzNFPOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Base Model using 8 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "7ycR0B4CzUSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CVErveOYFOXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "8FDztdnv0RCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "IvqOxYfk0RnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Khởi động lại phiên làm việc của bạn!\n",
        "\n",
        "Để tải mô hình tiếp theo và xóa bộ nhớ đệm của mô hình trước đó, bạn cần vào Runtime >> Restart session và chạy lại các cell ban đầu (import và đăng nhập HuggingFace).\n",
        "\n",
        "Việc này giúp làm sạch GPU."
      ],
      "metadata": {
        "id": "Pb7-0OfSEycl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# Load the Tokenizer and the Base Model using 4 bit\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "1FfMJ2JbzEr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model"
      ],
      "metadata": {
        "id": "mjp1EHH10WTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
      ],
      "metadata": {
        "id": "wSpavkXo1KOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "o3kXoy3w1oMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model"
      ],
      "metadata": {
        "id": "U2PL1nlM1tM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
        "# These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
        "# Let's count the number of weights using their dimensions:\n",
        "\n",
        "# See the matrix dimensions above\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "# Each layer comes to\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "# There are 32 layers\n",
        "params = lora_layer * 32\n",
        "\n",
        "# So the total size in MB is\n",
        "size = (params * 4) / 1_000_000\n",
        "\n",
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ],
      "metadata": {
        "id": "IIaqo-gyBQRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKnCsfUQBG-P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}